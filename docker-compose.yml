# cheeseshirt docker-compose
#
# CI/CD: GitHub Actions builds and pushes images to GHCR on merge to main.
#        See .github/workflows/build-and-push.yml
#
# Production deployment:
#   docker compose pull
#   docker compose up -d
#
# Local development (build from source):
#   docker compose up -d --build
#
# Note: The 'build' directives exist for local dev convenience only.
# Production should always pull pre-built images from GHCR - never build on the server.
#
# =============================================================================
# LLM Configuration
# =============================================================================
# The Monger talks to an LLM sidecar service. Enable ONE of:
#
#   1. llm-openai:        Cloud API (~80MB image)
#                         Requires OPENAI_API_KEY
#                         Fast responses, pay per token
#
#   2. llm-ollama:        CPU-only local inference (~100MB image)
#                         Model pulled at runtime, stored in volume
#                         First startup slow, subsequent fast
#
#   3. llm-ollama-phi3.5: CPU-only with model baked in (~2.3GB image)
#                         Instant startup, no runtime downloads
#                         Best for production deployments
#
# To switch LLMs, comment/uncomment the appropriate service below.
# The Monger doesn't know or care which LLM is being used.
# =============================================================================

services:
  # ==========================================================================
  # LLM Sidecar - Enable ONE of these (Option 1, 2, or 3)
  # ==========================================================================

  # ---------------------------------------------------------------------------
  # Option 1: OpenAI (cloud API)
  # ---------------------------------------------------------------------------
  # Smallest image, requires API key, charges per token
  # 
  # llm:
  #   image: ghcr.io/contrived-com/cheeseshirt-llm-openai:latest
  #   build:
  #     context: ./llm-openai
  #     dockerfile: Dockerfile
  #   container_name: cheeseshirt-llm
  #   restart: unless-stopped
  #   environment:
  #     - OPENAI_API_KEY=${OPENAI_API_KEY}
  #     - MODEL_NAME=${LLM_MODEL:-gpt-4o}
  #     - DEFAULT_TEMPERATURE=${LLM_TEMPERATURE:-0.7}
  #     - DEFAULT_MAX_TOKENS=${LLM_MAX_TOKENS:-1024}
  #     - LOG_LEVEL=${LOG_LEVEL:-info}
  #   healthcheck:
  #     test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:11435/health').raise_for_status()"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 5s
  #   networks:
  #     - cheeseshirt

  # ---------------------------------------------------------------------------
  # Option 2: Ollama CPU-only (model pulled at runtime) - ACTIVE
  # ---------------------------------------------------------------------------
  # Uses alpine/ollama (~22MB base), model downloaded on first start
  # Model is pinned to specific version for reproducibility
  llm:
    image: ghcr.io/contrived-com/cheeseshirt-llm-ollama:latest
    build:
      context: ./llm-ollama
      dockerfile: Dockerfile
    container_name: cheeseshirt-llm
    restart: unless-stopped
    environment:
      # Pinned to specific quantization for reproducibility
      - MODEL_NAME=phi3.5:3.8b-mini-instruct-q4_K_M
      - DEFAULT_TEMPERATURE=${LLM_TEMPERATURE:-0.7}
      - DEFAULT_MAX_TOKENS=${LLM_MAX_TOKENS:-1024}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      # Keep model loaded in RAM indefinitely (don't unload after idle)
      - OLLAMA_KEEP_ALIVE=-1
    volumes:
      # Persist downloaded models between restarts
      - ollama-models:/root/.ollama
    healthcheck:
      test: ["CMD", "wget", "-q", "-O-", "http://localhost:11435/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s  # Model download can take a while on first start
    networks:
      - cheeseshirt

  # ---------------------------------------------------------------------------
  # Option 3: Ollama CPU-only with phi3.5 baked in (fat image)
  # ---------------------------------------------------------------------------
  # ~2.3GB image with model pre-downloaded, instant startup
  # Best for production when you want consistent, fast cold starts
  #
  # llm:
  #   image: ghcr.io/contrived-com/cheeseshirt-llm-ollama-phi3.5:latest
  #   build:
  #     context: ./llm-ollama-phi3.5
  #     dockerfile: Dockerfile
  #   container_name: cheeseshirt-llm
  #   restart: unless-stopped
  #   environment:
  #     - MODEL_NAME=phi3.5:3.8b-mini-instruct-q4_K_M
  #     - DEFAULT_TEMPERATURE=${LLM_TEMPERATURE:-0.7}
  #     - DEFAULT_MAX_TOKENS=${LLM_MAX_TOKENS:-1024}
  #     - LOG_LEVEL=${LOG_LEVEL:-info}
  #   healthcheck:
  #     test: ["CMD", "wget", "-q", "-O-", "http://localhost:11435/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s  # Fast startup since model is baked in
  #   networks:
  #     - cheeseshirt

  # ==========================================================================
  # Core Services
  # ==========================================================================

  monger:
    image: ghcr.io/contrived-com/cheeseshirt-monger:latest
    build:
      context: ./monger
      dockerfile: Dockerfile
    container_name: cheeseshirt-monger
    restart: unless-stopped
    depends_on:
      llm:
        condition: service_healthy
    environment:
      - HOST=0.0.0.0
      - PORT=3002
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - LOG_PATH=/app/logs/cheeseshirt-monger.log
      - CHARACTER_CONFIG_PATH=/app/config/monger.json
      # LLM sidecar connection
      - LLM_SERVICE_URL=http://llm:11435
      - LLM_SERVICE_TIMEOUT=120.0
    volumes:
      - ${LOG_PATH:-./logs}:/app/logs
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:3002/health').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    networks:
      - cheeseshirt

  api:
    image: ghcr.io/contrived-com/cheeseshirt-api:latest
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: cheeseshirt-api
    restart: unless-stopped
    depends_on:
      monger:
        condition: service_healthy
    environment:
      - NODE_ENV=production
      - PORT=3001
      - HOST=0.0.0.0
      - MONGER_SERVICE_URL=http://monger:3002
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY}
      - STRIPE_PUBLISHABLE_KEY=${STRIPE_PUBLISHABLE_KEY}
      - STRIPE_WEBHOOK_SECRET=${STRIPE_WEBHOOK_SECRET}
      - SHIRT_PRICE_CENTS=${SHIRT_PRICE_CENTS:-3500}
      - SITE_URL=${SITE_URL:-https://cheeseshirt.com}
      - CONVERSATIONS_PATH=/app/data/conversations
      - COOKIE_SECURE=${COOKIE_SECURE:-false}
      - TIME_WASTER_THRESHOLD_HOURS=${TIME_WASTER_THRESHOLD_HOURS:-24}
      - LOG_PATH=/app/logs/cheeseshirt-api.log
      - LOG_LEVEL=${LOG_LEVEL}
    volumes:
      - ${LOG_PATH:-./logs}:/app/logs
      - ${DATA_PATH:-./data}/conversations:/app/data/conversations
    ports:
      - "127.0.0.1:3001:3001"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:3001/api/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s
    networks:
      - cheeseshirt

  web:
    image: ghcr.io/contrived-com/cheeseshirt-web:latest
    build:
      context: ./web
      dockerfile: Dockerfile
    container_name: cheeseshirt-web
    restart: unless-stopped
    ports:
      - "127.0.0.1:8080:8080"  
    depends_on:
      api:
        condition: service_healthy
    volumes:
      - ${LOG_PATH:-./logs}:/app/logs
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:8080/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s
    networks:
      - cheeseshirt

  # ==========================================================================
  # Background Workers
  # ==========================================================================

  worker-stripe-orders:
    image: ghcr.io/contrived-com/cheeseshirt-worker-stripe-orders:latest
    build:
      context: ./worker-stripe-orders
      dockerfile: Dockerfile
    container_name: cheeseshirt-worker-stripe-orders
    restart: unless-stopped
    environment:
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY}
      - POLL_INTERVAL_SECONDS=${STRIPE_POLL_INTERVAL:-300}
      - ORDERS_DIR=/app/data/orders
      - STATE_DIR=/app/data/state
      - CONVERSATIONS_DIR=/app/data/conversations
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ${DATA_PATH:-./data}/orders:/app/data/orders
      - ${DATA_PATH:-./data}/state:/app/data/state
      - ${DATA_PATH:-./data}/conversations:/app/data/conversations:ro
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8002/health').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - cheeseshirt

networks:
  cheeseshirt:
    driver: bridge

volumes:
  ollama-models:
